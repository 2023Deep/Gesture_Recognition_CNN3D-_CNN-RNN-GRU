# -*- coding: utf-8 -*-
"""gitlab Gesture_Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FvGRVsBeUm2Gyu22e-Rc_onRMhd46Ylf

# Gesture Recognition

## Objective:
  * Develop a feature for smart TVs to recognize five different user gestures for controlling the TV without a remote.

## Description:
  * The goal is to enable users to interact with the smart TV using hand gestures.
  
  * Five specific gestures need to be recognized accurately for controlling different functions of the TV.

## Scope:

  * Implement machine learning algorithms to recognize gestures accurately.
  * Ensure real-time processing for seamless user experience.

## Definition of Gestures

  * The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:
    
    * Thumbs up: Increase the volume
    * Thumbs down: Decrease the volume
    * Left swipe: 'Jump' backwards 10 seconds
    * Right swipe: 'Jump' forward 10 seconds
    * Stop: Pause the movie

## Understanding the Dataset
  
  * The training data consists of a few hundred videos categorised into one of the five classes.
  
  * Each video (typically 2-3 seconds long) is divided into a **sequence of 30 frames(images)**.
  
  * These videos have been recorded by various people performing one of the five gestures in front of a webcam - similar to what the smart TV will use.
    
## Folder Structure
  
  * The data is in a zip file. The zip file contains a 'train' and a 'val' folder with two CSV files for the two folders.
  
  * These folders are in turn divided into subfolders where each subfolder represents a video of a particular gesture. Each subfolder, i.e. a video, contains 30 frames (or images).
  
  * All images in a particular video subfolder have the same dimensions but different videos may have different dimensions. Specifically, videos have two types of dimensions - either 360x360 or 120x160 (depending on the webcam used to record the videos).
  
  * Hence, we will need to do some pre-processing to standardise the videos.
  
  * Each row of the CSV file represents one video and contains three main pieces of information - the name of the subfolder containing the 30 images of the video, the name of the gesture and the numeric label (between 0-4) of the video.

## Approach

  * The objective  is to train a model on the 'train' folder which performs well on the 'val' folder as well (as usually done in ML projects).
  * We have withheld the test folder for evaluation purposes - your final model's performance will be tested on the 'test' set.

## Types of architectures

* For analysing videos using neural networks, **two types of architectures** are used commonly. One is the standard **CNN + RNN architecture** in which we pass the images of a video through a CNN which extracts a feature vector for each image, and then pass the sequence of these feature vectors through an RNN. This is something we are already familiar with (in theory).

* The other popular architecture used to process videos is a natural extension of CNNs - a **3D convolutional network**.

* In this project, we will try both these architectures.

## Experiment Summary

For this case study, 10 experiments have been done as mentioned below.

Due to limited resources, I'll be running 5 epochs initially and for experiment 6 I'll increase it to 10 and try to infer the results.

- Experiment 1: Model with 3 Conv3d  + Batch Normalisation + Pooling layers
- Experiment 2: Increasing Layer Set on Experiment 1
- Experiment 3: Increasing Dense Layer Kernels on Experiment 2
- Experiment 4: Increasing batch size on Experiment3
- Experiment 5: Increasing learning rate on Experiment 4
- Experiment 6: Increasing epochs to 8 on Experiment 3
- Experiment 7: CNN+RNN with MobileNetV2 Transfer Learning + GRU layers
- Experiment 8: Decreasing the number of output neurons from MobileNet on Experiment 7
- Experiment 9: Decreasing batch size and learning rate, increasing GRU Kernels


From the experiments done below, **the best chosen model is Experiment 1** which has best training and validation accuracy of about 76% each
"""

#mounting google drive
from google.colab import drive
drive.mount('/content/drive')

#importing data
# Replace with your actual shareable link (ensure correct permissions)
shareable_link = "https://drive.google.com/file/d/13aDFoWCXeFicE-FU3khD2jWKUVZL_R5x/view?usp=sharing"

# Assuming the zip file is at the root of your mounted Drive (adjust if needed)
zip_file_path = "/content/drive/MyDrive/Project_data.zip"

# Target folder to store unzipped data (replace with your desired name)
target_folder_path = "/content/Project_data_unzipped"

!unzip -qn {zip_file_path} -d {target_folder_path}

import numpy as np
import os
from imageio import imread
from skimage.transform import resize
import datetime
import matplotlib.pyplot as plt
import os

"""We set the random seed so that the results don't vary drastically."""

np.random.seed(30)
import random as rn
rn.seed(30)
import tensorflow as tf
from tensorflow import keras
tf.random.set_seed(30)
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, GRU, Dropout, Flatten, TimeDistributed, BatchNormalization, MaxPooling3D, GlobalAveragePooling3D, Conv3D, Activation
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras import optimizers
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, MobileNetV2

## Checking the GPU configuration

!nvidia-smi

"""In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."""

train_doc = np.random.permutation(open('/content/Project_data_unzipped/Project_data/train.csv').readlines())
val_doc = np.random.permutation(open('/content/Project_data_unzipped/Project_data/val.csv').readlines())

# setting frame indexes to be used for training
img_idx = range(4,22)

batch_size = 32
# each frame width set to be 120x120 for CNN3D since frames are of sizes 120x160 and 360x360
frame_width =120
frame_height=120

"""## Generator
This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy.
"""

def generator(source_path, folder_list, batch_size, transfer_learning=False):
    # print( 'Source path = ', source_path, '; batch size =', batch_size)
    global img_idx  #create a list of image numbers you want to use for a particular video
    while True:
        t = np.random.permutation(folder_list)

        num_batches = len(t) // batch_size
        for batch in range(num_batches): # we iterate over the number of batches

            x, y, z = len(img_idx), frame_width, frame_height

            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB
            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output

            for folder in range(batch_size): # iterate over the batch_size
                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder
                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in
                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)
                    if transfer_learning:
                        # MobileNetV2 proprocessing function
                        image = preprocess_input(image)

                    #crop the images and resize them
                    image_width, image_height = image.shape[0], image.shape[1]
                    top_offset, left_offset, bottom_offset, right_offset = int(image_height / 2) -60, int(image_width/2) -60, int(image_height / 2) +60, int(image_width / 2) +60
                    image = image[left_offset : right_offset, top_offset : bottom_offset, :]

                    if image.shape[1] == 160:
                        image = resize(image[:,20:140,:],(frame_width, frame_height)).astype(np.float32)
                    else:
                        image = resize(image,(frame_width, frame_height)).astype(np.float32)
                    # Normalizing the image in respective channels
                    batch_data[folder,idx,:,:,0] = (image[:,:,0] - np.mean(image[:,:,0])) / np.std(image[:,:,0])
                    batch_data[folder,idx,:,:,1] = (image[:,:,1] - np.mean(image[:,:,1])) / np.std(image[:,:,1])
                    batch_data[folder,idx,:,:,2] = (image[:,:,2] - np.mean(image[:,:,2])) / np.std(image[:,:,2])

                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1
            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do


        count = len(t) % batch_size

        if count != 0:
            # write the code for the remaining data points which are left after full batches
            batch_labels = np.zeros((count,5)) # batch_labels is the one hot representation of the output
            batch_data = np.zeros((count,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB

            for folder in range(count): # iterate over the count
                imgs = os.listdir(source_path+'/'+ t[folder + (batch_size * num_batches)].split(';')[0]) # read all the images in the folder
                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in

                    image = imread(source_path+'/'+ t[folder + (batch_size * num_batches)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)
                    if transfer_learning:
                        # MobileNetV2 proprocessing function
                        image = preprocess_input(image)

                    #crop the images and resize them
                    image_width, image_height = image.shape[0], image.shape[1]
                    top_offset, left_offset, bottom_offset, right_offset = int(image_height / 2) -60, int(image_width/2) -60, int(image_height / 2) +60, int(image_width / 2) +60
                    image = image[left_offset : right_offset, top_offset : bottom_offset, :]

                    if image.shape[1] == 160:
                        image = resize(image[:,20:140,:],(frame_width, frame_height)).astype(np.float32)
                    else:
                        image = resize(image,(frame_width, frame_height)).astype(np.float32)

                    # Normalizing the image in respective channels
                    batch_data[folder,idx,:,:,0] = (image[:,:,0] - np.mean(image[:,:,0])) / np.std(image[:,:,0])
                    batch_data[folder,idx,:,:,1] = (image[:,:,1] - np.mean(image[:,:,1])) / np.std(image[:,:,1])
                    batch_data[folder,idx,:,:,2] = (image[:,:,2] - np.mean(image[:,:,2])) / np.std(image[:,:,2])

                batch_labels[folder, int(t[folder + (batch_size * num_batches)].strip().split(';')[2])] = 1

            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do

"""Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."""



curr_dt_time = datetime.datetime.now()
train_path = '/content/Project_data_unzipped/Project_data/train'
val_path = '/content/Project_data_unzipped/Project_data/val'

num_train_sequences = len(train_doc)
print('# training sequences =', num_train_sequences)
num_val_sequences = len(val_doc)
print('# validation sequences =', num_val_sequences)
num_epochs = 5
print ('# epochs =', num_epochs)

'''
Function to display the training vs validation accuracy and training vs validation losses over epochs
'''
def plot_curves(history):
    acc = history.history['categorical_accuracy']
    val_acc = history.history['val_categorical_accuracy']

    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs_range = range(history.params['epochs'])

    plt.figure(figsize=(16, 8))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')

    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.show()

"""## Model
Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam.

Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.

Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`.

The `steps_per_epoch` and `validation_steps` are used by `fit` method to decide the number of next() calls it need to make.

Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch.
"""

# transfer_learning parameter not given since CNN3D wont use it
train_generator = generator(train_path, train_doc, batch_size)
val_generator = generator(val_path, val_doc, batch_size)

model_name = 'model_init_cnn' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'

if not os.path.exists(model_name):
    os.mkdir(model_name)

filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}.h5'

# Defining checkpoint callback
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')
# Defining early stop callback
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',patience=3, mode='min')
# defnining reduceLR callback
LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', cooldown=0, min_lr=0.0001)
callbacks_list = [checkpoint, LR]

if (num_train_sequences%batch_size) == 0:
    steps_per_epoch = int(num_train_sequences/batch_size)
else:
    steps_per_epoch = (num_train_sequences//batch_size) + 1

if (num_val_sequences%batch_size) == 0:
    validation_steps = int(num_val_sequences/batch_size)
else:
    validation_steps = (num_val_sequences//batch_size) + 1

# Input shape = (18, 120, 120, 3)
input_shape = (len(img_idx), frame_width, frame_height, 3)

"""## Experiment 1"""

# Define model
model = Sequential()

model.add(Conv3D(8, kernel_size=(3,3,3), input_shape=input_shape, padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling3D(pool_size=(2,2,2)))

model.add(Conv3D(16, kernel_size=(3,3,3), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling3D(pool_size=(2,2,2)))

model.add(Conv3D(64, kernel_size=(1,3,3), padding='same'))
model.add(Activation('relu'))
model.add(Dropout(0.25))
model.add(MaxPooling3D(pool_size=(2,2,2)))

#Flatten Layers
model.add(Flatten())

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

#softmax layer
model.add(Dense(5, activation='softmax'))

# Using SGD optimizer with default LR=0.01
optimiser = keras.optimizers.SGD()
model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])
print (model.summary())
# Fitting the model
history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,
                    callbacks=callbacks_list, validation_data=val_generator,
                    validation_steps=validation_steps, class_weight=None, workers=-1, initial_epoch=0)

# Produce a graph for Accuracy and losses
plot_curves(history)

"""**Observation**

the model is performing okay as both training and validation accuracy but need to increase the epochs of traing (limited resoureces) . We will try adding more layers to extract more features

## Experiment 2: Increasing Layer Set
"""

# Define model
model = Sequential()

model.add(Conv3D(8, kernel_size=(3,3,3), input_shape=input_shape, padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling3D(pool_size=(2,2,2)))

model.add(Conv3D(16, kernel_size=(3,3,3), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling3D(pool_size=(2,2,2)))

# Increased layer set
model.add(Conv3D(32, kernel_size=(1,3,3), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling3D(pool_size=(2,2,2)))

model.add(Conv3D(64, kernel_size=(1,3,3), padding='same'))
model.add(Activation('relu'))
model.add(Dropout(0.25))
model.add(MaxPooling3D(pool_size=(2,2,2)))

#Flatten Layers
model.add(Flatten())

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

#softmax layer
model.add(Dense(5, activation='softmax'))

# Using SGD optimizer with default LR=0.01
optimiser = keras.optimizers.SGD() #write your optimizer
model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])
print (model.summary())

# Fitting the model
history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,
                    callbacks=callbacks_list, validation_data=val_generator,
                    validation_steps=validation_steps, class_weight=None, workers=-1, initial_epoch=0)

# Produce a graph for Accuracy and losses
plot_curves(history)

"""**Observations**

After adding more layers, the number of parameters dropped sharply. Hence the model is now underfitting. Both the training and validation accuracy are now under 0.55.
So we shall add more Kernels in dense layer of this model and see the output.

## Experiment 3: Increasing Dense Layer Kernels
"""

# Define model
model = Sequential()

model.add(Conv3D(8, kernel_size=(3,3,3), input_shape=input_shape, padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling3D(pool_size=(2,2,2)))

model.add(Conv3D(16, kernel_size=(3,3,3), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling3D(pool_size=(2,2,2)))

model.add(Conv3D(32, kernel_size=(1,3,3), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling3D(pool_size=(2,2,2)))

model.add(Conv3D(64, kernel_size=(1,3,3), padding='same'))
model.add(Activation('relu'))
model.add(Dropout(0.25))
model.add(MaxPooling3D(pool_size=(2,2,2)))

#Flatten Layers
model.add(Flatten())

model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))

#softmax layer
model.add(Dense(5, activation='softmax'))

optimiser = keras.optimizers.SGD() #write your optimizer
model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])
print (model.summary())

# Fitting the model
history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,
                    callbacks=callbacks_list, validation_data=val_generator,
                    validation_steps=validation_steps, class_weight=None, workers=-1, initial_epoch=0)

# Produce a graph for Accuracy and losses
plot_curves(history)

"""**Observations**

This model is performing very well as both validation and training accuracies are increasing steadily and the losses are also decreasing steadily. Also unlike Experiment 1, the number of parameters are much lower as well.

It looks like it is not able to converge in 30 epochs so we shall try increasing speed with increasing batch size.

## Experiment 4: Increasing batch size
"""

batch_size=64

train_generator = generator(train_path, train_doc, batch_size)
val_generator = generator(val_path, val_doc, batch_size)

if (num_train_sequences%batch_size) == 0:
    steps_per_epoch = int(num_train_sequences/batch_size)
else:
    steps_per_epoch = (num_train_sequences//batch_size) + 1

if (num_val_sequences%batch_size) == 0:
    validation_steps = int(num_val_sequences/batch_size)
else:
    validation_steps = (num_val_sequences//batch_size) + 1

optimiser = keras.optimizers.SGD() #write your optimizer
model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])
print (model.summary())

history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,
                    callbacks=callbacks_list, validation_data=val_generator,
                    validation_steps=validation_steps, class_weight=None, workers=-1, initial_epoch=0)

# Produce a graph for Accuracy and losses
plot_curves(history)

"""**Observations**

The training and validation accuracies both struggle in the beginning but start to increase very slowly later. We shall try increasing the learning rate as well and check if the model starts performing better

## Experiment 5: Increasing learning rate
"""

num_epochs = 5

# Using SGD optimizer with LR=0.02 due to double batch size
optimiser = keras.optimizers.SGD(learning_rate=0.02)
model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])
print (model.summary())

# Fitting the model
history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,
                    callbacks=callbacks_list, validation_data=val_generator,
                    validation_steps=validation_steps, class_weight=None, workers=-1, initial_epoch=0)

# Produce a graph for Accuracy and losses
plot_curves(history)

"""**Observations**

Increasing learning rate did not help acheieve the desired output. The model now overfits a lot. We shall try to increase the number of epochs of model and bring down the batch size to 32 and see if it is a better one

## Experiment 6: Increasing epochs to 8
"""

num_epochs = 8

batch_size=32
train_generator = generator(train_path, train_doc, batch_size)
val_generator = generator(val_path, val_doc, batch_size)

if (num_train_sequences%batch_size) == 0:
    steps_per_epoch = int(num_train_sequences/batch_size)
else:
    steps_per_epoch = (num_train_sequences//batch_size) + 1

if (num_val_sequences%batch_size) == 0:
    validation_steps = int(num_val_sequences/batch_size)
else:
    validation_steps = (num_val_sequences//batch_size) + 1

# Define model
model = Sequential()

model.add(Conv3D(8, kernel_size=(3,3,3), input_shape=input_shape, padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling3D(pool_size=(2,2,2)))

model.add(Conv3D(16, kernel_size=(3,3,3), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling3D(pool_size=(2,2,2)))

model.add(Conv3D(32, kernel_size=(1,3,3), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling3D(pool_size=(2,2,2)))

model.add(Conv3D(64, kernel_size=(1,3,3), padding='same'))
model.add(Activation('relu'))
model.add(Dropout(0.25))
model.add(MaxPooling3D(pool_size=(2,2,2)))

#Flatten Layers
model.add(Flatten())

model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))

#softmax layer
model.add(Dense(5, activation='softmax'))

# Defining SGD optimizer with default LR=0.01
optimiser = keras.optimizers.SGD()
model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])
print (model.summary())

# Fitting the model
history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,
                    callbacks=callbacks_list, validation_data=val_generator,
                    validation_steps=validation_steps, class_weight=None, workers=-1, initial_epoch=0)

# Produce a graph for Accuracy and losses
plot_curves(history)



"""# CNN+RNN

We can also use another architecture of using CNN to extract features and RNN to process the extracted features in a time distributed fashion. This way we can acheieve a video processing NN. For this task, we can use pre-trained models and use the power of transfer learning. We shall choose MobileNetV2 CNN model since it is lightweight in nature which would suit the task in hand for application.

Since MobileNetV2 requires input to be processed in a specific format and shape we shall use preprocess_input() from keras mobilenet_v2 package and resize image to 128x128.
"""

frame_width =128
frame_height=128

# using transfer learning so passing True
train_generator = generator(train_path, train_doc, batch_size, True)
val_generator = generator(val_path, val_doc, batch_size, True)

model_name = 'model_init_gru' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'

if not os.path.exists(model_name):
    os.mkdir(model_name)

filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}.h5'

# Adding checkpoint Callback
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto',save_freq = num_epochs)
# Adding ReduceLR callback
LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', cooldown=0, min_lr=0.00001)
callbacks_list = [checkpoint, LR]

"""## Experiment 7: CNN+RNN with MobileNetV2 Transfer Learning + GRU layers"""

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, GRU, Dropout, Flatten, TimeDistributed
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras import optimizers
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, MobileNetV2

base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(128,128,3))
x = base_model.output
x = Flatten()(x)
#x.add(Dropout(0.5))
features = Dense(24, activation='relu')(x)
conv_model = Model(inputs=base_model.input, outputs=features)
# Disable trainability of layers in MobileNetV2
for layer in base_model.layers:
    layer.trainable = False

model = Sequential()
model.add(TimeDistributed(conv_model, input_shape=(len(img_idx), 128, 128, 3)))
model.add(GRU(32, return_sequences=True))
model.add(GRU(16))

model.add(Dense(100, activation='relu'))
model.add(Dense(5, activation='softmax'))

# Compiling model with SGD optimizer with default LR of 0.01
model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
model.build()
print (model.summary())

history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,
                    callbacks=callbacks_list, validation_data=val_generator,
                    validation_steps=validation_steps, class_weight=None, workers=-1, initial_epoch=0)

# Produce a graph for Accuracy and losses
plot_curves(history)

"""**Observations**

We can see that out model is overfitting since training accuracy is reaching  above 90% and validation accuracy is stagnant at 0.65. So we shall try adding dropout layer to mobilenetv2.

## Experiment 8: Decreasing the number of output neurons from MobileNet
"""

base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(128,128,3))
x = base_model.output
x = Flatten()(x)
# adding dropout for reducing neurons of MobileNet to combat oveerfitting
x = Dropout(0.5)(x)
# Adding dense layer of 24 kernels
features = Dense(24, activation='relu')(x)
conv_model = Model(inputs=base_model.input, outputs=features)
# Disable trainability of layers in MobileNetV2
for layer in base_model.layers:
    layer.trainable = False

model = Sequential()
model.add(TimeDistributed(conv_model, input_shape=(len(img_idx), 128, 128, 3)))
model.add(GRU(32, return_sequences=True))
model.add(GRU(16))

model.add(Dense(100, activation='relu'))
model.add(Dense(5, activation='softmax'))

# Compiling model with SGD optimizer with default LR of 0.01
model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
model.build()
print (model.summary())

history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,
                    callbacks=callbacks_list, validation_data=val_generator,
                    validation_steps=validation_steps, class_weight=None, workers=-1, initial_epoch=0)

# Produce a graph for Accuracy and losses
plot_curves(history)

"""**Observations**

The model is still overfitting and there has been no improvement. We will now try combining multiple changes with some increase in parameters and some decrease. Also we shall tweak optimser as well

## Experiment 9: Decreasing batch size and learning rate, increasing GRU Kernels
"""

batch_size = 16

# using transfer learning so passing True
train_generator = generator(train_path, train_doc, batch_size, True)
val_generator = generator(val_path, val_doc, batch_size, True)

if (num_train_sequences%batch_size) == 0:
    steps_per_epoch = int(num_train_sequences/batch_size)
else:
    steps_per_epoch = (num_train_sequences//batch_size) + 1

if (num_val_sequences%batch_size) == 0:
    validation_steps = int(num_val_sequences/batch_size)
else:
    validation_steps = (num_val_sequences//batch_size) + 1

base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(128,128,3))
x = base_model.output
x = Flatten()(x)
# Adding dense layer of 64 kernels
features = Dense(64, activation='relu')(x)
conv_model = Model(inputs=base_model.input, outputs=features)
# Disable trainability of layers in MobileNetV2
for layer in base_model.layers:
    layer.trainable = False

model = Sequential()
model.add(TimeDistributed(conv_model, input_shape=(len(img_idx), 128, 128, 3)))
model.add(GRU(64, return_sequences=True))
model.add(GRU(32))

model.add(Dense(1000, activation='relu'))
model.add(Dense(5, activation='softmax'))

# Compiling model with SGD optimizer with  LR of 0.001, decay of 0.00006 and momentum=0.7
# Using legacy SGD optimizer
sgd = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.7, nesterov=True)
model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])
print(model.summary())

# Fitting the model
history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,
                    callbacks=callbacks_list, validation_data=val_generator,
                    validation_steps=validation_steps, class_weight=None, workers=-1, initial_epoch=0)

# Produce a graph for Accuracy and losses
plot_curves(history)

"""**Observations**

The model has a high tendency of overfitting. Adding dropouts and the learning rate reduction didnt help much. There is no improvement.

# Summary

The Experiment number 1 with CNN3D yield the best result in terms of training and validation accuracies.
Although there is further room for improvement which can be explored.
"""

